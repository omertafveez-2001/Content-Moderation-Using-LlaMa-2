# Fine-Tuning LLaMa-2 Using Ludwig.AI Architecture
In this report, we will be discussing fine-tuning LLaMa-2 (60B Parameters)  for Content Moderation and Toxicity Classification trained on the dataset downloaded from JigSaw Competition on Kaggle, integrated with Ludwig.AI architecture designed by Uber for Declarative Machine. 
The advent of Large Language Models (LLMs) has reshaped multi-label classification, with Meta's LLaMa standing out for its remarkable ability to capture intricate semantic relationships and contextualized representations. LLaMa's fine-tuning paradigm, leveraging pre-existing knowledge, allows it to efficiently adapt to multi-label tasks with reduced data requirements. This specialization for the target task establishes LLaMa as a preferred choice, offering a comparative advantage over traditional deep-learning approaches. It strikes a robust balance between generalization and task-specific adaptation, positioning itself as a top performer in multi-label classification scenarios.

Ludwig Architecture presents an efficient, declarative framework, transcending conventional limitations in machine learning. Marked by a modular structure, it encompasses independent components for tasks like data loading, preprocessing, and model training. The use of a declarative YAML file simplifies model specification, enhancing expressiveness. Ludwig introduces features like automatic hyperparameter tuning and meta-learning, making it a versatile framework for diverse machine-learning challenges.

Fine-tuning Large Language Models (LLMs) becomes imperative due to constraints associated with extensive data and the computational demands of training all parameters from scratch. This conventional approach is not only expensive but also impractically slow. Fine-tuning LLMs offers an efficient strategy, allowing pre-trained models to adapt to custom datasets, particularly beneficial for achieving exceptional performance in domain-specific tasks with limited data. In the literature, fine-tuning stands out as a pivotal and pragmatic methodology for optimizing language models. Fine-tuning models such as LLaMa-60B is a very efficient way of fine-tuning a smaller model on a custom dataset that performs as well as the larger model. However, this comes with the expense of computation power and storage. In general, fine-tuning requires the use of GPUs with more than 16Gb of storage, making Fine-tuning on large models computationally expensive.
![ludwig_legos_unanimated](https://github.com/omertafveez-2001/Finetuning-LlaMA-2/assets/64263186/a96d132e-10e8-45a9-a8a8-a24f0135335a)

## Dataset and Preprocessing
         
The dataset, sourced from Kaggle's Jigsaw Competition, comprises text and binary labels representing sentiment classes. To accommodate a sequence-to-sequence (seq2seq) architecture in Large Language Models (LLMs), binary labels were converted into string format ("toxic: label severe_toxic: label obscene: label threat: label insult: label identity_hate: label"). The dataset structure, inspired by Alpaca Dataset, includes an additional column with consistent instructions for each row, aligning with both seq2seq expectations and established conventions.

‘’’ Recognize the Sentiment Polarity of the Class. Given the classes determine whether the text belongs to the class or not. If the text belongs to the class assign the class 1 else 0. Here are the classes: [Toxic, Severely Toxic, Obscene, Threat, Insult, Identity Hate]. Answer Format:
Toxic: class_value
Severely Toxic: class_value
Obscene: class_value
Threat: class_value
Insult: class_value
Identity Hate: class_value’’’

The inclusion of instructions in the dataset, coupled with input and output data, plays a pivotal role in enhancing the context for fine-tuning Large Language Models (LLMs). Instructions provide explicit guidance on the desired behavior or specific task objectives, offering a form of meta-information that refines the model's understanding during training. In preprocessing, addressing high-class imbalance was crucial, leading to the use of oversampling techniques. Balancing the dataset is imperative to mitigate challenges like bias, reduced generalization, misleading metrics, inadequate embeddings, and hyperparameter tuning intricacies. Employing oversampling is pivotal, aligning with established practices in the literature to foster robust and unbiased model training. This also aids in managing token size constraints. The imperative to balance the dataset stems from the recognition that class imbalance introduces multifaceted challenges, encompassing issues such as bias, diminished generalization, misleading evaluation metrics, insufficient representations in embeddings, and intricacies in hyperparameter tuning

## Changing Ludwig's Encoder-Decoder Data Pipeline
Within the official release of the Ludwig API, a notable absence is observed in the provision for set-based text outputs, mirroring the structure present in the post-preprocessing dataset. To address this limitation, recourse to the developer's API access is taken, allowing for a nuanced modification of the data pipeline within the model architecture. This adaptation involves the utilization of decorator functions, strategically employed to register parameters and configurations specific to encoding set-type features. These parameters extend to encompass considerations for both the decoder and loss functions associated with set-type features. In configuring this codebase, the implementation of Mixin structures becomes evident, serving as a pivotal organizational mechanism for encapsulating and reusing common configurations across distinct segments of the Ludwig framework. This strategic use of Mixin aligns with established practices in the literature, demonstrating a systematic approach to enhancing the flexibility and adaptability of the Ludwig API for diverse data structures and model requirements.

Set features have one encoder: embed, the raw binary values coming from the input placeholders are first transformed to sparse integer lists, then they are mapped to either dense or sparse embeddings (one-hot encodings), finally they are reduced on the sequence dimension and returned as an aggregated embedding vector. Inputs are of size b while outputs are of size b x h where b is the batch size and h is the dimensionality of the embeddings.

## Prompt Templating and Zero-Shot Inference
Zero-shot learning is a capability enabled by Large Language Models, allowing them to generalize to tasks or domains they have never been explicitly trained on. It involves presenting the model with a task description or prompt along with some context, and expecting it to generate a relevant response or output. We have used zero-shot inference the evaluate the model’s performance on domain-specific tasks before the fine-tuning phase.
For this purpose, we used Ludwig’s architecture to control prompting and quality of generation during zero-shot inference. 

From the results, we observed that the base LLaMa-2 model is generally good at producing coherent English text. However, when it does not know how to respond, it just returns the input over and over again until the token limit is reached. It does not know when to stop producing a response, it gets confused and just produces till sets a hard stop through the number of maximum allowed tokens. It does not get even one of the input prompts correct.

Therefore, we see the need for fine-tuning and why LLaMa-2 and other LLMs models cannot perform well on direct inference using zero-shot learning.

## Declarative Fine-Tuning LLaMA-2

The initial pre-training phase involves training a language model on a massive corpus of text data to learn general language patterns and representations. Fine-tuning, on the other hand, customizes the model to a specific task or domain by exposing it to task-specific data. This typically allows the model to perform better and achieve higher accuracy on the specific task compared to using the pre-trained model by itself.
To make a pre-trained model do better at these specific tasks, we can train it on examples of those tasks. This is known as instruction fine-tuning. This training process typically changes the underlying model weights. 

However, fine-tuning comes with a lot of memory constraints. Fine-tuning a 60B parameter model is extremely difficult and computationally expensive. By default bit configuration, each parameter is 32-bit, which means for a 7B parameter, this would consume approximately 28Gb of Memory. Furthermore, each parameter has one distinct gradient, which means an additional 28Gb, totaling up to 54Gb of Memory. The use of start-of-the-art optimizers such as Adam and AdamW also uses high memory usage. For a 7B parameter, the memory would exceed 112Gb, which is massive compared to general use RAM capacity of 16Gb. In addressing computational efficiency concerns, the adoption of quantized training emerges as a strategic solution, wherein a continuous space of floating-point numbers is discretized into a finite number of bins, effectively halving the memory requirement from 32-bit to int8. Notably, within this framework, the integration of LoRA, an auxiliary matrix, assumes significance. LoRA introduces a nuanced approach to parameter updates during backpropagation, selectively updating only a fraction of the parameters rather than the entire model parameter set. This selective updating mechanism aims to optimize computational resources while maintaining model performance. Furthermore, the deployment of QLoRA underscores a commitment to further quantization, specifically reducing the original 16-bit Transformer to as low as a 4-bit Transformer. This reduction is achieved by transforming the data types of the original parameters into 4 bits, showcasing a meticulous approach to model compression and efficiency enhancement within the training process. These techniques align with prevailing literature on quantized training, showcasing a systematic exploration of methodologies to optimize computational resources without compromising model effectiveness.

